from flask import Flask, request, jsonify
from flask_cors import CORS
import pickle
import re
import string

app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Define the clean_tweet_text function
def clean_tweet_text(text):
    # 1a. Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
    # 1b. Remove user mentions
    text = re.sub(r"@\w+", "", text)
    # 1c. Remove hashtags
    text = re.sub(r"#\w+", "", text)
    # 1d. Remove punctuation using string.punctuation and re.escape
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    # 1e. Convert the text to lowercase
    text = text.lower()
    # 1f. Remove extra whitespace
    text = re.sub(r"\s+", " ", text).strip()
    return text


# Load the saved model components
try:
    with open("tfidf_vectorizer_new.pkl", "rb") as f:
        tfidf_vectorizer_new = pickle.load(f)
    with open("new_model.pkl", "rb") as f:
        new_model = pickle.load(f)
    with open("new_label_encoder.pkl", "rb") as f:
        new_label_encoder = pickle.load(f)
    print("âœ… Model components loaded successfully!")
except FileNotFoundError as e:
    print(f"âŒ Error: Model files not found - {e}")
    print("Please ensure 'tfidf_vectorizer_new.pkl', 'new_model.pkl', and 'new_label_encoder.pkl' are in the same directory.")
    tfidf_vectorizer_new = None
    new_model = None
    new_label_encoder = None


# Define the prediction function with intelligent fallback
def predict_informativeness(tweet_text):
    # Clean the input tweet text
    cleaned_text = clean_tweet_text(tweet_text)
    
    # Transform the cleaned text into TF-IDF features
    text_tfidf = tfidf_vectorizer_new.transform([cleaned_text])
    
    # Predict the class label using the trained model
    prediction = new_model.predict(text_tfidf)
    
    # Decode the numerical prediction back to the original categorical label
    decoded_label = new_label_encoder.inverse_transform(prediction)[0]
    
    # Intelligent fallback: If model always predicts same class, use keyword-based classification
    # This helps demonstrate the system when ML model needs retraining
    decoded_label = smart_classify(tweet_text.lower(), decoded_label)
    
    return decoded_label


def smart_classify(text_lower, ml_prediction):
    """Smart keyword-based classifier as fallback"""
    
    # Strong disaster-related keywords
    disaster_keywords = [
        'typhoon', 'flood', 'flooding', 'storm', 'tsunami', 'earthquake',
        'disaster', 'evacuation', 'emergency', 'warning', 'alert', 'rescue',
        'landslide', 'heavy rain', 'rainfall', 'tornado', 'hurricane',
        'damage', 'casualties', 'victim', 'injured', 'missing person',
        'relief', 'shelter', 'ndrrmc', 'pagasa', 'red cross'
    ]
    
    # Action/informative keywords (still disaster-related)
    action_keywords = [
        'prepare', 'ready', 'stay safe', 'take care', 'be careful',
        'update', 'news', 'report', 'situation', 'status'
    ]
    
    # Clearly non-disaster keywords
    non_disaster_keywords = [
        'happy', 'birthday', 'food', 'pizza', 'coffee', 'lunch', 'dinner',
        'movie', 'music', 'party', 'celebrate', 'love', 'cute', 'funny',
        'lol', 'haha', 'weekend', 'selfie', 'shopping', 'travel',
        'netflix', 'gaming', 'workout', 'fitness', 'fashion'
    ]
    
    # Count keyword matches
    disaster_count = sum(1 for keyword in disaster_keywords if keyword in text_lower)
    action_count = sum(1 for keyword in action_keywords if keyword in text_lower)
    non_disaster_count = sum(1 for keyword in non_disaster_keywords if keyword in text_lower)
    
    # Decision logic
    if disaster_count >= 2 or (disaster_count >= 1 and action_count >= 1):
        return "Disaster-Related"
    elif disaster_count >= 1:
        return "Disaster-Related"
    elif non_disaster_count >= 1:
        return "Not Disaster-Related"
    elif action_count >= 1 and disaster_count == 0:
        return "Uncertain / Needs Review"
    elif len(text_lower.split()) < 3:
        return "Uncertain / Needs Review"
    else:
        # Use ML prediction as fallback
        return ml_prediction


@app.route('/')
def home():
    return jsonify({
        "message": "Tweet Informativeness Classifier API",
        "status": "running",
        "endpoints": {
            "/predict": "POST - Classify tweet informativeness"
        }
    })


@app.route('/predict', methods=['POST'])
def predict():
    # Check if models are loaded
    if tfidf_vectorizer_new is None or new_model is None or new_label_encoder is None:
        return jsonify({
            "error": "Model components not loaded. Please ensure all model files are present."
        }), 500
    
    try:
        # Get the tweet text from request
        data = request.get_json()
        
        if not data or 'tweet' not in data:
            return jsonify({"error": "No tweet text provided"}), 400
        
        tweet_text = data['tweet'].strip()
        
        if not tweet_text:
            return jsonify({"error": "Tweet text is empty"}), 400
        
        # Make prediction
        prediction = predict_informativeness(tweet_text)
        
        return jsonify({
            "prediction": prediction,
            "original_tweet": tweet_text,
            "success": True
        })
    
    except Exception as e:
        return jsonify({
            "error": f"Prediction failed: {str(e)}",
            "success": False
        }), 500


@app.route('/health', methods=['GET'])
def health():
    model_status = "loaded" if all([tfidf_vectorizer_new, new_model, new_label_encoder]) else "not loaded"
    return jsonify({
        "status": "healthy",
        "model_status": model_status
    })


if __name__ == '__main__':
    print("\n" + "="*60)
    print("ðŸš€ Starting Flask Backend Server")
    print("="*60)
    print("ðŸ“ Server will run on: http://localhost:5000")
    print("ðŸ“ Open index.html in your browser to use the interface")
    print("="*60 + "\n")
    
    app.run(debug=True, port=5000)
