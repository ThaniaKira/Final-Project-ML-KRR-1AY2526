{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LGjFc0PyP7NI"},"outputs":[],"source":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"524aa432","executionInfo":{"status":"ok","timestamp":1765883746384,"user_tz":-480,"elapsed":34,"user":{"displayName":"Exor","userId":"08370747278105835313"}},"outputId":"e22649b3-0bb8-4943-cd26-04b6a970c0e7"},"source":["import pandas as pd\n","\n","# Load the CSV file into a DataFrame\n","df = pd.read_csv('/content/2013_Typhoon_Yolanda-tweets_labeled.csv')\n","\n","# Display the first 5 rows of the DataFrame\n","print(\"First 5 rows of the DataFrame:\")\n","print(df.head())\n","\n","# Print the DataFrame's information\n","print(\"\\nDataFrame Information:\")\n","df.info()\n","\n","# Display the value counts for the 'Informativeness' column, as 'label' does not exist\n","print(\"\\nValue counts for 'Informativeness' column:\")\n","print(df[' Informativeness'].value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows of the DataFrame:\n","             Tweet ID                                         Tweet Text  \\\n","0  397748184581025793  May super typhoon pala this weekend, Paano na ...   \n","1  397781298577559552  RT @ChikkanessAve: Ariella Arida on her swimwe...   \n","2  397915554066857985  Tropical storm Haiyan intensifies into typhoon...   \n","3  397978405720686592  Super Typhoon Yolanda be good sa mga taga Boho...   \n","4  398015038759247872  RT @MsCandice_UC: Advisory- As of 2am Nov 6 #Y...   \n","\n","   Information Source      Information Type                Informativeness  \n","0           Outsiders  Sympathy and support  Related - but not informative  \n","1         Not labeled           Not labeled                    Not related  \n","2               Media    Caution and advice        Related and informative  \n","3           Outsiders  Sympathy and support  Related - but not informative  \n","4           Outsiders    Caution and advice        Related and informative  \n","\n","DataFrame Information:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1048 entries, 0 to 1047\n","Data columns (total 5 columns):\n"," #   Column               Non-Null Count  Dtype \n","---  ------               --------------  ----- \n"," 0   Tweet ID             1048 non-null   int64 \n"," 1    Tweet Text          1048 non-null   object\n"," 2    Information Source  1048 non-null   object\n"," 3    Information Type    1048 non-null   object\n"," 4    Informativeness     1048 non-null   object\n","dtypes: int64(1), object(4)\n","memory usage: 41.1+ KB\n","\n","Value counts for 'Informativeness' column:\n"," Informativeness\n","Related and informative          765\n","Related - but not informative    175\n","Not related                       71\n","Not applicable                    37\n","Name: count, dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"fa2dc62a"},"source":["## Preprocess Tweet Text\n","\n","### Subtask:\n","Clean the tweet text by removing noise such as URLs, user mentions, hashtags, punctuation, and converting text to lowercase. This prepares the text for feature extraction.\n"]},{"cell_type":"markdown","metadata":{"id":"21e1646b"},"source":["**Reasoning**:\n","To clean the tweet text as per the instructions, I will define a Python function that uses regular expressions and string methods to remove URLs, user mentions, hashtags, punctuation, convert text to lowercase, and remove extra whitespace. I will then apply this function to the ' Tweet Text' column to create a new 'cleaned_text' column.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8707b276","executionInfo":{"status":"ok","timestamp":1765883746444,"user_tz":-480,"elapsed":62,"user":{"displayName":"Exor","userId":"08370747278105835313"}},"outputId":"d99b1eee-7f32-44e7-eb8c-8958834554da"},"source":["import re\n","import string # Import the string module\n","\n","def clean_tweet_text(text):\n","    # 1a. Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    # 1b. Remove user mentions\n","    text = re.sub(r'@\\w+', '', text)\n","    # 1c. Remove hashtags\n","    text = re.sub(r'#\\w+', '', text)\n","    # 1d. Remove punctuation using string.punctuation and re.escape\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n","    # 1e. Convert the text to lowercase\n","    text = text.lower()\n","    # 1f. Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Apply the cleaning function to the ' Tweet Text' column\n","df['cleaned_text'] = df[' Tweet Text'].apply(clean_tweet_text)\n","\n","# Display the first few rows of the DataFrame with original and cleaned text\n","print(\"First 5 rows with original and cleaned tweet text:\")\n","print(df[[' Tweet Text', 'cleaned_text']].head())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows with original and cleaned tweet text:\n","                                          Tweet Text  \\\n","0  May super typhoon pala this weekend, Paano na ...   \n","1  RT @ChikkanessAve: Ariella Arida on her swimwe...   \n","2  Tropical storm Haiyan intensifies into typhoon...   \n","3  Super Typhoon Yolanda be good sa mga taga Boho...   \n","4  RT @MsCandice_UC: Advisory- As of 2am Nov 6 #Y...   \n","\n","                                        cleaned_text  \n","0  may super typhoon pala this weekend paano na k...  \n","1  rt ariella arida on her swimwear during the mi...  \n","2  tropical storm haiyan intensifies into typhoon...  \n","3  super typhoon yolanda be good sa mga taga boho...  \n","4  rt advisory as of 2am nov 6 hasnt entered phil...  \n"]}]},{"cell_type":"markdown","metadata":{"id":"8451aa68"},"source":["## Feature Engineering for Text Classification\n","\n","### Subtask:\n","Convert the preprocessed text data into numerical features suitable for machine learning. This will involve using a TF-IDF Vectorizer to transform text into a matrix of TF-IDF features.\n"]},{"cell_type":"markdown","metadata":{"id":"c228eff6"},"source":["**Reasoning**:\n","To convert the cleaned text into numerical features and prepare the target variable, I will import `TfidfVectorizer` and `LabelEncoder`, initialize both, fit and transform the `cleaned_text` column into TF-IDF features, and encode the ' Informativeness' column into numerical labels.\n","\n"]},{"cell_type":"code","metadata":{"id":"10e1d6f5","executionInfo":{"status":"ok","timestamp":1765883746728,"user_tz":-480,"elapsed":279,"user":{"displayName":"Exor","userId":"08370747278105835313"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9159c713-5421-4502-a7df-5804adbb4d6e"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Initialize TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limiting to 5000 features for practical reasons\n","\n","# Fit and transform the 'cleaned_text' column\n","X = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n","\n","# Prepare the target variable 'Informativeness'\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(df[' Informativeness'])\n","\n","print(\"Shape of TF-IDF features (X):\", X.shape)\n","print(\"First 5 encoded target labels (y):\", y[:5])\n","print(\"Original labels and their encoded values:\")\n","for i, label in enumerate(label_encoder.classes_):\n","    print(f\"{label}: {i}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of TF-IDF features (X): (1048, 3040)\n","First 5 encoded target labels (y): [2 1 3 2 3]\n","Original labels and their encoded values:\n","Not applicable: 0\n","Not related: 1\n","Related - but not informative: 2\n","Related and informative: 3\n"]}]},{"cell_type":"markdown","metadata":{"id":"354a8ae3"},"source":["## Train Classification Model\n","\n","### Subtask:\n","Split the dataset into training and testing sets. Train a suitable text classification model (e.g., Logistic Regression or a Support Vector Machine) using the TF-IDF features to identify Taglish/Filipino disaster tweets.\n"]},{"cell_type":"markdown","metadata":{"id":"a90bbb69"},"source":["**Reasoning**:\n","To prepare the data for model training, I will split the TF-IDF features and target labels into training and testing sets using `train_test_split` and then train a Logistic Regression model as instructed.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f91edce8","executionInfo":{"status":"ok","timestamp":1765883746752,"user_tz":-480,"elapsed":42,"user":{"displayName":"Exor","userId":"08370747278105835313"}},"outputId":"0b2af2d4-b5c4-4007-f3af-641006c3d692"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train a Logistic Regression model\n","# Using 'liblinear' solver for better convergence with small datasets and sparse data\n","model = LogisticRegression(solver='liblinear', random_state=42)\n","model.fit(X_train, y_train)\n","\n","print(\"Shapes of the split data:\")\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shapes of the split data:\n","X_train shape: (838, 3040)\n","X_test shape: (210, 3040)\n","y_train shape: (838,)\n","y_test shape: (210,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"4db5f161"},"source":["## Evaluate Model Performance\n","\n","### Subtask:\n","Evaluate the performance of the trained classification model using appropriate metrics such as accuracy, precision, recall, and F1-score on the test set to assess its effectiveness.\n"]},{"cell_type":"markdown","metadata":{"id":"3db489a8"},"source":["**Reasoning**:\n","To evaluate the model's performance, I will make predictions on the test set, import the required metrics from `sklearn.metrics`, calculate the accuracy, and generate a detailed classification report.\n","\n"]},{"cell_type":"code","metadata":{"id":"08d650a5","executionInfo":{"status":"ok","timestamp":1765883746757,"user_tz":-480,"elapsed":34,"user":{"displayName":"Exor","userId":"08370747278105835313"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7a74880a-bcfe-4f71-b203-8f0513e2547d"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy:.4f}\")\n","\n","# Generate and print a detailed classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 0.7190\n","\n","Classification Report:\n","                               precision    recall  f1-score   support\n","\n","               Not applicable       0.00      0.00      0.00         2\n","                  Not related       0.00      0.00      0.00        17\n","Related - but not informative       0.67      0.05      0.09        41\n","      Related and informative       0.72      0.99      0.83       150\n","\n","                     accuracy                           0.72       210\n","                    macro avg       0.35      0.26      0.23       210\n","                 weighted avg       0.64      0.72      0.61       210\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"code","metadata":{"id":"abc1b369","executionInfo":{"status":"ok","timestamp":1765884821918,"user_tz":-480,"elapsed":23,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02c4fb91-4c5f-49b6-b5f7-76244b90edd5"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy:.4f}\")\n","\n","# Generate and print a detailed classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 0.0095\n","\n","Classification Report:\n","                               precision    recall  f1-score   support\n","\n","               Not applicable       0.00      0.00      0.00       150\n","                  Not related       0.00      0.00      0.00        19\n","Related - but not informative       0.67      0.05      0.09        41\n","      Related and informative       0.00      0.00      0.00         0\n","\n","                     accuracy                           0.01       210\n","                    macro avg       0.17      0.01      0.02       210\n","                 weighted avg       0.13      0.01      0.02       210\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"6160556f"},"source":["### Debugging `NotFittedError` Step-by-Step"]},{"cell_type":"markdown","metadata":{"id":"e843a4a2"},"source":["#### 1. Verify Python Version Compatibility\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3a71353","executionInfo":{"status":"ok","timestamp":1765886093207,"user_tz":-480,"elapsed":18,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"ed8a27e1-bcc5-4461-e61c-683f3ebedb27"},"source":["import sys\n","print(f\"Local Python Version: {sys.version}\")\n","\n","# In Colab, you can run this to see the Colab Python version\n","# print(f\"Colab Python Version: {sys.version}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Local Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"bf5d914e"},"source":["#### 2. Inspect the Loaded `TfidfVectorizer` Locally\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4616664f","executionInfo":{"status":"ok","timestamp":1765886093801,"user_tz":-480,"elapsed":57,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"ae8ae3b6-f0ca-4f65-983b-71aa04f81494"},"source":["# This code block is meant to be run LOCALLY, NOT in Colab.\n","# Save this as `check_vectorizer.py` and run `python check_vectorizer.py` locally.\n","\n","import pickle\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","try:\n","    with open('tfidf_vectorizer_new.pkl', 'rb') as f:\n","        loaded_vectorizer = pickle.load(f)\n","    print(\"Successfully loaded tfidf_vectorizer_new.pkl\")\n","\n","    # Check if the vectorizer is fitted by looking for the `idf_` attribute\n","    if hasattr(loaded_vectorizer, 'idf_'):\n","        print(\"Vectorizer appears to be fitted (has 'idf_' attribute).\")\n","        print(f\"Number of features (vocabulary size): {len(loaded_vectorizer.vocabulary_)}\")\n","        print(f\"First 5 IDF values: {loaded_vectorizer.idf_[:5]}\")\n","    else:\n","        print(\"WARNING: Vectorizer does NOT appear to be fitted (missing 'idf_' attribute).\")\n","\n","except FileNotFoundError:\n","    print(\"Error: tfidf_vectorizer_new.pkl not found. Make sure it's in the same directory.\")\n","except Exception as e:\n","    print(f\"An error occurred while loading or inspecting the vectorizer: {e}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded tfidf_vectorizer_new.pkl\n","Vectorizer appears to be fitted (has 'idf_' attribute).\n","Number of features (vocabulary size): 3040\n","First 5 IDF values: [7.26244543 7.26244543 6.85698032 5.12237926 5.75836803]\n"]}]},{"cell_type":"markdown","metadata":{"id":"6caab12a"},"source":["The output of `check_vectorizer.py` will tell us definitively if the loaded object is considered fitted by `sklearn`. If it still reports \"WARNING: Vectorizer does NOT appear to be fitted,\" then the issue is with the `.pkl` file itself (meaning it wasn't saved correctly in Colab, or you're loading an old version).\n","\n","If it *does* appear fitted, then the problem lies elsewhere in your local Streamlit app's execution, perhaps with how `df['cleaned_text']` is being prepared before transformation, or an unexpected interaction with another library."]},{"cell_type":"markdown","metadata":{"id":"82d81f44"},"source":["#### 3. Ensure `df['cleaned_text']` is consistent\n","\n","The `tfidf_vectorizer_new` was fitted using the `df['cleaned_text']` column. If the text cleaning function (`clean_tweet_text`) behaves differently in your local environment (e.g., different `re` module behavior, string encodings), it could lead to an issue. However, `NotFittedError` is specific to the vectorizer's internal state, not necessarily the input data format. Still, it's worth double-checking.\n","\n","Make sure the `clean_tweet_text` function in your local `app.py` is identical to the one in Colab."]},{"cell_type":"code","metadata":{"id":"13b8cd70","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765884018873,"user_tz":-480,"elapsed":7342,"user":{"displayName":"Exor","userId":"08370747278105835313"}},"outputId":"87b440b4-fa4e-46ca-fe1f-71b3a1713522"},"source":["!pip install streamlit\n","import streamlit as st\n","import pickle\n","\n","# Save the necessary components for Streamlit to load\n","# The clean_tweet_text function needs to be available in the Streamlit app script directly\n","with open('tfidf_vectorizer.pkl', 'wb') as f:\n","    pickle.dump(tfidf_vectorizer, f)\n","with open('model.pkl', 'wb') as f:\n","    pickle.dump(model, f)\n","with open('label_encoder.pkl', 'wb') as f:\n","    pickle.dump(label_encoder, f)\n","\n","# Streamlit app code will be generated in the next step"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.52.1)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"]}]},{"cell_type":"code","source":["!pip install streamlit pyngrok"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FjBMumQh4oLG","executionInfo":{"status":"ok","timestamp":1765883954597,"user_tz":-480,"elapsed":11651,"user":{"displayName":"Exor","userId":"08370747278105835313"}},"outputId":"d555585a-490b-46d5-f9cd-4727741f335d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.52.1-py3-none-any.whl.metadata (9.8 kB)\n","Collecting pyngrok\n","  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Downloading streamlit-1.52.1-py3-none-any.whl (9.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n","Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyngrok, pydeck, streamlit\n","Successfully installed pydeck-0.9.1 pyngrok-7.5.0 streamlit-1.52.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"0b9e1abd"},"source":["## Summary:\n","\n","### Q&A\n","The developed intelligent expert system is designed to classify the informativeness of\n","tweets related to the 2013 Typhoon Yolanda disaster. Its capabilities include identifying whether a given tweet is 'Related and informative', 'Related - but not informative', 'Not related', or 'Not applicable'. The system utilizes a Logistic Regression model trained on TF-IDF features extracted from cleaned tweet text. The interactive web application, built with Gradio, provides a simple interface where users can input tweet text and instantly receive a prediction of its informativeness category from the trained model.\n","\n","### Data Analysis Key Findings\n","*   The dataset, comprising 1048 tweets, was analyzed, revealing four informativeness categories, with 'Related and informative' being the most prevalent (765 out of 1048 tweets).\n","*   Tweet texts underwent preprocessing, including the removal of URLs, user mentions, hashtags, punctuation, and conversion to lowercase, resulting in a `cleaned_text` column.\n","*   The cleaned text was transformed into 3040 numerical TF-IDF features for each tweet, serving as input for the classification model.\n","*   A Logistic Regression model was trained and achieved an overall accuracy of 71.90% on the test set.\n","*   The model demonstrated strong performance in identifying 'Related and informative' tweets, with a precision of 0.72, recall of 0.99, and an F1-score of 0.83.\n","*   Conversely, the model performed poorly on the less frequent categories: 'Not applicable' and 'Not related' tweets received a precision, recall, and F1-score of 0.00, indicating a complete failure to correctly predict any instances of these classes. The 'Related - but not informative' category also showed very low recall (0.05) and F1-score (0.09).\n","*   An interactive web application was successfully created using Gradio, allowing real-time classification of user-entered tweet text into one of the four informativeness categories.\n","\n","### Insights or Next Steps\n","*   **Address Class Imbalance:** The severe imbalance in the dataset led to the model's inability to classify minority classes effectively. Future work should implement techniques such as oversampling (e.g., SMOTE), undersampling, or using class weighting during model training to improve the detection of 'Not applicable', 'Not related', and 'Related - but not informative' tweets.\n","*   **Explore Advanced Models and Features:** Consider experimenting with more sophisticated machine learning models (e.g., Support Vector Machines, Random Forests, or deep learning architectures like LSTMs or Transformer models) and more advanced feature engineering, such as word embeddings (Word2Vec, GloVe) or contextual embeddings (BERT), to potentially capture more nuanced linguistic patterns and improve overall classification performance across all categories.\n"]},{"cell_type":"markdown","metadata":{"id":"d9c4caea"},"source":["# Task\n","The previous task was approved. I will now proceed with mapping the existing 'Informativeness' labels to new classification categories.\n","\n","# Map Labels to New Classification Categories\n","\n","### Subtask:\n","Create a new target column by mapping the existing 'Informativeness' labels to the new classification categories: 'Disaster-Related', 'Not Disaster-Related', and 'Uncertain / Needs Review'. Specifically, 'Related and informative' will be mapped to 'Disaster-Related'. 'Not related' and 'Not applicable' will be mapped to 'Not Disaster-Related'. 'Related - but not informative' will be mapped to 'Uncertain / Needs Review'. Finally, encode these new string labels into numerical values.\n","\n","### Reasoning:\n","To achieve the new classification objective, I will first define a mapping dictionary based on the specified criteria. Then, I will create a new column, `new_labels`, in the DataFrame by applying this mapping to the existing ' Informativeness' column. Finally, I will use a `LabelEncoder` to transform these new categorical string labels into numerical values, which are required for model training. This will generate the new target variable `y_new`."]},{"cell_type":"markdown","metadata":{"id":"a42dcbb0"},"source":["## Map Labels to New Classification Categories\n","\n","### Subtask:\n","Create a new target column by mapping the existing 'Informativeness' labels to the new classification categories: 'Disaster-Related', 'Not Disaster-Related', and 'Uncertain / Needs Review'. Specifically, 'Related and informative' will be mapped to 'Disaster-Related'. 'Not related' and 'Not applicable' will be mapped to 'Not Disaster-Related'. 'Related - but not informative' will be mapped to 'Uncertain / Needs Review'. Finally, encode these new string labels into numerical values.\n"]},{"cell_type":"code","metadata":{"id":"d8a9965f","executionInfo":{"status":"ok","timestamp":1765884028428,"user_tz":-480,"elapsed":169,"user":{"displayName":"Exor","userId":"08370747278105835313"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b95419a7-c5b9-4489-cbe0-696beed848a2"},"source":["import pandas as pd\n","import re\n","import string\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Re-load the DataFrame as it was not defined in the current context\n","df = pd.read_csv('/content/2013_Typhoon_Yolanda-tweets_labeled.csv')\n","\n","# Re-define the clean_tweet_text function as it was used to prepare df\n","def clean_tweet_text(text):\n","    # 1a. Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    # 1b. Remove user mentions\n","    text = re.sub(r'@\\w+', '', text)\n","    # 1c. Remove hashtags\n","    text = re.sub(r'#\\w+', '', text)\n","    # 1d. Remove punctuation using string.punctuation and re.escape\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n","    # 1e. Convert the text to lowercase\n","    text = text.lower()\n","    # 1f. Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Re-apply the cleaning function to the ' Tweet Text' column to get 'cleaned_text'\n","df['cleaned_text'] = df[' Tweet Text'].apply(clean_tweet_text)\n","\n","# 1. Create a dictionary that maps the original 'Informativeness' labels to the new categories\n","label_mapping = {\n","    'Related and informative': 'Disaster-Related',\n","    'Not related': 'Not Disaster-Related',\n","    'Not applicable': 'Not Disaster-Related',\n","    'Related - but not informative': 'Uncertain / Needs Review'\n","}\n","\n","# 2. Apply this mapping to the ' Informativeness' column to create a new column named `new_labels`\n","df['new_labels'] = df[' Informativeness'].map(label_mapping)\n","\n","# 3. Initialize a new LabelEncoder\n","new_label_encoder = LabelEncoder()\n","\n","# 4. Fit and transform the `new_labels` column to convert string labels into numerical values\n","y_new = new_label_encoder.fit_transform(df['new_labels'])\n","\n","# 5. Print the value counts of the `new_labels` column\n","print(\"\\nValue counts for the new_labels column:\")\n","print(df['new_labels'].value_counts())\n","\n","# 6. Print the first few entries of `y_new` and the mapping of the new numerical labels\n","print(\"\\nFirst 5 encoded new target labels (y_new):\")\n","print(y_new[:5])\n","print(\"\\nOriginal new labels and their encoded values:\")\n","for i, label in enumerate(new_label_encoder.classes_):\n","    print(f\"{label}: {i}\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Value counts for the new_labels column:\n","new_labels\n","Disaster-Related            765\n","Uncertain / Needs Review    175\n","Not Disaster-Related        108\n","Name: count, dtype: int64\n","\n","First 5 encoded new target labels (y_new):\n","[2 1 0 2 0]\n","\n","Original new labels and their encoded values:\n","Disaster-Related: 0\n","Not Disaster-Related: 1\n","Uncertain / Needs Review: 2\n"]}]},{"cell_type":"markdown","metadata":{"id":"4fb5e647"},"source":["## Prepare Text Features\n","\n","### Subtask:\n","Re-use the already cleaned 'cleaned_text' column and the pre-fitted TF-IDF vectorizer to transform the text data into numerical features (X) for model training.\n"]},{"cell_type":"markdown","metadata":{"id":"1a2b4c0e"},"source":["**Reasoning**:\n","To prepare the text features for model training, I will reuse the `tfidf_vectorizer` from a previous step to transform the `cleaned_text` column into numerical features and store them in `X`, then print the shape of `X`.\n","\n"]},{"cell_type":"code","metadata":{"id":"2c4ff1d0"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Initialize TfidfVectorizer if not already initialized (should be from previous steps)\n","# For demonstration purposes, re-initializing here if it's the first execution after a restart\n","# In a continuous notebook execution, tfidf_vectorizer would already be in memory\n","if 'tfidf_vectorizer' not in locals() or tfidf_vectorizer is None:\n","    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","    # Assuming df and cleaned_text are available from previous steps\n","    tfidf_vectorizer.fit(df['cleaned_text'])\n","\n","# Transform the 'cleaned_text' column into TF-IDF features using the pre-fitted vectorizer\n","X = tfidf_vectorizer.transform(df['cleaned_text'])\n","\n","# Print the shape of the newly generated X\n","print(\"Shape of TF-IDF features (X):\")\n","print(X.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a205f986"},"source":["## Train New Classification Model\n","\n","### Subtask:\n","Split the TF-IDF features (X) and the newly mapped and encoded target labels (y_new) into training and testing sets. Then, initialize and train a Logistic Regression model using these new training sets.\n"]},{"cell_type":"markdown","metadata":{"id":"703a753e"},"source":["**Reasoning**:\n","To prepare the data for model training and then train a Logistic Regression model, I will split the `X` (TF-IDF features) and `y_new` (new target labels) into training and testing sets, initialize a Logistic Regression model, train it, and then print the shapes of the split datasets.\n","\n"]},{"cell_type":"code","metadata":{"id":"fcb83787"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","# Split the dataset into training and testing sets using X and y_new\n","X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=0.2, random_state=42)\n","\n","# Initialize and train a Logistic Regression model\n","# Using 'liblinear' solver for better convergence with small datasets and sparse data\n","new_model = LogisticRegression(solver='liblinear', random_state=42)\n","new_model.fit(X_train, y_train)\n","\n","print(\"Shapes of the split data for new labels:\")\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"446b436d"},"source":["## Evaluate New Model Performance\n","\n","### Subtask:\n","Make predictions on the test set using the newly trained model. Evaluate its performance by calculating accuracy and generating a detailed classification report with precision, recall, and F1-score for the new classification categories, ensuring 'zero_division' is handled appropriately.\n"]},{"cell_type":"markdown","metadata":{"id":"e4423c0b"},"source":["**Reasoning**:\n","To evaluate the performance of the newly trained model, I will make predictions on the test set, calculate its accuracy, and generate a detailed classification report, handling zero division in the report.\n","\n"]},{"cell_type":"code","metadata":{"id":"f19b65e3"},"source":["from sklearn.metrics import accuracy_score, classification_report\n","\n","# Make predictions on the test set using the new_model\n","y_pred_new = new_model.predict(X_test)\n","\n","# Calculate and print accuracy for the new model\n","accuracy_new = accuracy_score(y_test, y_pred_new)\n","print(f\"New Model Accuracy: {accuracy_new:.4f}\")\n","\n","# Generate and print a detailed classification report for the new model\n","print(\"\\nNew Classification Report:\")\n","print(classification_report(y_test, y_pred_new, target_names=new_label_encoder.classes_, zero_division=0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2944178d"},"source":["## Develop Interactive Web Application\n","\n","### Subtask:\n","Save the TF-IDF vectorizer, the trained Logistic Regression model, and the new label encoder. Then, create a Streamlit application script that loads these components and provides an interactive interface for users to input tweet text and receive a classification (Disaster-Related, Not Disaster-Related, or Uncertain / Needs Review) based on the new model.\n"]},{"cell_type":"markdown","metadata":{"id":"06394cde"},"source":["**Reasoning**:\n","To prepare for the Streamlit application, I will save the `tfidf_vectorizer`, the `new_model`, and the `new_label_encoder` to pickle files as instructed.\n","\n"]},{"cell_type":"code","metadata":{"id":"dedb24e7","executionInfo":{"status":"ok","timestamp":1765884720905,"user_tz":-480,"elapsed":21,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"21af0360-5ee7-4856-ff78-78fc833e04c6"},"source":["import pickle\n","from sklearn.feature_extraction.text import TfidfVectorizer # Ensure TfidfVectorizer is available\n","\n","# Re-initialize and re-fit TfidfVectorizer before saving to ensure it's in a fitted state.\n","# This explicitly addresses the 'NotFittedError' by guaranteeing the vectorizer is fitted\n","# at the point of serialization, regardless of previous cell execution order or state.\n","# Assuming df and cleaned_text are available from previous steps.\n","# Using the same parameters (max_features) as in previous fitting steps.\n","tfidf_vectorizer_to_save = TfidfVectorizer(max_features=5000)\n","tfidf_vectorizer_to_save.fit(df['cleaned_text'])\n","\n","\n","# 1. Save the tfidf_vectorizer object (using the newly fitted one)\n","with open('tfidf_vectorizer_new.pkl', 'wb') as f:\n","    pickle.dump(tfidf_vectorizer_to_save, f)\n","print(\"Saved tfidf_vectorizer_new.pkl\")\n","\n","# 2. Save the newly trained new_model object\n","with open('new_model.pkl', 'wb') as f:\n","    pickle.dump(new_model, f)\n","print(\"Saved new_model.pkl\")\n","\n","# 3. Save the new_label_encoder object\n","with open('new_label_encoder.pkl', 'wb') as f:\n","    pickle.dump(new_label_encoder, f)\n","print(\"Saved new_label_encoder.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved tfidf_vectorizer_new.pkl\n","Saved new_model.pkl\n","Saved new_label_encoder.pkl\n"]}]},{"cell_type":"markdown","metadata":{"id":"5280ebe0"},"source":["**Reasoning**:\n","Now that the necessary components (vectorizer, model, and label encoder) are saved, I will create the Streamlit application script as specified in the instructions. This script will load these components, define the cleaning and prediction functions, and set up the interactive web interface.\n","\n"]},{"cell_type":"code","metadata":{"id":"5fa73589"},"source":["import sys\n","!{sys.executable} -m pip install streamlit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c121dee5"},"source":["import streamlit as st\n","import pickle\n","import re\n","import string\n","\n","# Define the clean_tweet_text function (must be available in the Streamlit app)\n","def clean_tweet_text(text):\n","    # 1a. Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    # 1b. Remove user mentions\n","    text = re.sub(r'@\\w+', '', text)\n","    # 1c. Remove hashtags\n","    text = re.sub(r'#\\w+', '', text)\n","    # 1d. Remove punctuation using string.punctuation and re.escape\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n","    # 1e. Convert the text to lowercase\n","    text = text.lower()\n","    # 1f. Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Load the saved components\n","# Assuming these files are in the same directory as the Streamlit app script\n","try:\n","    with open('tfidf_vectorizer_new.pkl', 'rb') as f:\n","        tfidf_vectorizer_new = pickle.load(f)\n","    with open('new_model.pkl', 'rb') as f:\n","        new_model = pickle.load(f)\n","    with open('new_label_encoder.pkl', 'rb') as f:\n","        new_label_encoder = pickle.load(f)\n","except FileNotFoundError:\n","    st.error(\"Model components not found. Please ensure 'tfidf_vectorizer_new.pkl', 'new_model.pkl', and 'new_label_encoder.pkl' are in the same directory.\")\n","    st.stop() # Stop the app if files are not found\n","\n","# Define the prediction function for Streamlit\n","def predict_new_informativeness(tweet_text):\n","    # Clean the input tweet text\n","    cleaned_text = clean_tweet_text(tweet_text)\n","\n","    # Transform the cleaned text into TF-IDF features\n","    text_tfidf = tfidf_vectorizer_new.transform([cleaned_text])\n","\n","    # Predict the class label using the trained model\n","    prediction = new_model.predict(text_tfidf)\n","\n","    # Decode the numerical prediction back to the original categorical label\n","    decoded_label = new_label_encoder.inverse_transform(prediction)[0]\n","\n","    return decoded_label\n","\n","# Streamlit app layout\n","st.title('New Tweet Informativeness Classifier')\n","st.write('Enter a tweet below to classify its informativeness regarding the 2013 Typhoon Yolanda into new categories.')\n","\n","user_input = st.text_area('Enter Tweet Text here:', '', height=150)\n","\n","if st.button('Classify Tweet'):\n","    if user_input:\n","        result = predict_new_informativeness(user_input)\n","        st.success(f'Predicted Informativeness: {result}')\n","    else:\n","        st.warning('Please enter some text to classify.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"408c125d","executionInfo":{"status":"ok","timestamp":1765884708668,"user_tz":-480,"elapsed":21,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1b16c73-3ce1-4817-e8ce-5720920aae66"},"source":["%%writefile app.py\n","import streamlit as st\n","import pickle\n","import re\n","import string\n","\n","# Define the clean_tweet_text function (must be available in the Streamlit app)\n","def clean_tweet_text(text):\n","    # 1a. Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    # 1b. Remove user mentions\n","    text = re.sub(r'@\\w+', '', text)\n","    # 1c. Remove hashtags\n","    text = re.sub(r'#\\w+', '', text)\n","    # 1d. Remove punctuation using string.punctuation and re.escape\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n","    # 1e. Convert the text to lowercase\n","    text = text.lower()\n","    # 1f. Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Load the saved components\n","# Assuming these files are in the same directory as the Streamlit app script\n","try:\n","    with open('tfidf_vectorizer_new.pkl', 'rb') as f:\n","        tfidf_vectorizer_new = pickle.load(f)\n","    with open('new_model.pkl', 'rb') as f:\n","        new_model = pickle.load(f)\n","    with open('new_label_encoder.pkl', 'rb') as f:\n","        new_label_encoder = pickle.load(f)\n","except FileNotFoundError:\n","    st.error(\"Model components not found. Please ensure 'tfidf_vectorizer_new.pkl', 'new_model.pkl', and 'new_label_encoder.pkl' are in the same directory.\")\n","    st.stop() # Stop the app if files are not found\n","\n","# Define the prediction function for Streamlit\n","def predict_new_informativeness(tweet_text):\n","    # Clean the input tweet text\n","    cleaned_text = clean_tweet_text(tweet_text)\n","\n","    # Transform the cleaned text into TF-IDF features\n","    text_tfidf = tfidf_vectorizer_new.transform([cleaned_text])\n","\n","    # Predict the class label using the trained model\n","    prediction = new_model.predict(text_tfidf)\n","\n","    # Decode the numerical prediction back to the original categorical label\n","    decoded_label = new_label_encoder.inverse_transform(prediction)[0]\n","\n","    return decoded_label\n","\n","# Streamlit app layout\n","st.title('New Tweet Informativeness Classifier')\n","st.write('Enter a tweet below to classify its informativeness regarding the 2013 Typhoon Yolanda into new categories.')\n","\n","user_input = st.text_area('Enter Tweet Text here:', '', height=150)\n","\n","if st.button('Classify Tweet'):\n","    if user_input:\n","        result = predict_new_informativeness(user_input)\n","        st.success(f'Predicted Informativeness: {result}')\n","    else:\n","        st.warning('Please enter some text to classify.')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"c0aa5216"},"source":["**Reasoning**:\n","Now that the Streamlit application code is written to `app.py`, I will run it in the background using `streamlit run` along with `ngrok` to create a public URL, fulfilling the subtask requirement of developing an interactive web application accessible in Colab.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"91542b69"},"source":["# Task\n","The plan is to update the existing text classification system by replacing the current dataset with '/content/2012_Philipinnes_floods-tweets_labeled.csv'. This involves reloading the data, applying the `clean_tweet_text` function to the ' Tweet Text' column to create 'cleaned_text', and then mapping the ' Informativeness' column to new classification categories ('Disaster-Related', 'Not Disaster-Related', 'Uncertain / Needs Review') and numerically encoding them into `y_new` using a new `LabelEncoder`. Next, the `TfidfVectorizer` will be re-initialized and re-fitted on the `cleaned_text` from the new dataset to generate updated TF-IDF features in `X`. A Logistic Regression model will then be trained on a split of these new `X` and `y_new` datasets. The model's performance will be evaluated using accuracy and a classification report. Finally, the re-fitted `tfidf_vectorizer`, the newly trained `new_model`, and the re-fitted `new_label_encoder` will be saved to pickle files ('tfidf_vectorizer_new.pkl', 'new_model.pkl', 'new_label_encoder.pkl') for deployment, and the entire process, including the updated performance and readiness of the Streamlit application, will be summarized."]},{"cell_type":"markdown","metadata":{"id":"215eda17"},"source":["## Load New Dataset\n","\n","### Subtask:\n","Load the new dataset from '/content/2012_Philipinnes_floods-tweets_labeled.csv' into a pandas DataFrame, replacing the previously used dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"0f34562a"},"source":["**Reasoning**:\n","Load the specified CSV file into a pandas DataFrame, then display its first 5 rows and print its information to understand its structure.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3c339fd","executionInfo":{"status":"ok","timestamp":1766855415047,"user_tz":-480,"elapsed":464,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"abeac178-20a0-40e7-fe27-965a5d078263"},"source":["import pandas as pd\n","\n","# Load the new CSV file into a DataFrame, replacing the old one\n","df = pd.read_csv('/content/2012_Philipinnes_floods-tweets_labeled.csv')\n","\n","# Display the first 5 rows of the DataFrame\n","print(\"First 5 rows of the new DataFrame:\")\n","print(df.head())\n","\n","# Print the DataFrame's information\n","print(\"\\nNew DataFrame Information:\")\n","df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows of the new DataFrame:\n","             Tweet ID                                         Tweet Text  \\\n","0  230634706436227073  Philippine floods worsen as death toll hits 14...   \n","1  230692092911964161  RT @gmanews: Philippine floods worsen as death...   \n","2  230882841498628096  Philippine flood fatalities hit 23 http://t.co...   \n","3  232034974851805187  4 Luzon dams release water; flood warning up: ...   \n","4  232098107490439169  RT @ANCALERTS: PAGASA 8:50pm advisory: Yellow ...   \n","\n","   Information Source          Information Type          Informativeness  \n","0               Media      Affected individuals  Related and informative  \n","1               Media      Affected individuals  Related and informative  \n","2               Media  Other Useful Information  Related and informative  \n","3               Media        Caution and advice  Related and informative  \n","4               Media        Caution and advice  Related and informative  \n","\n","New DataFrame Information:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 5 columns):\n"," #   Column               Non-Null Count  Dtype \n","---  ------               --------------  ----- \n"," 0   Tweet ID             1000 non-null   int64 \n"," 1    Tweet Text          1000 non-null   object\n"," 2    Information Source  1000 non-null   object\n"," 3    Information Type    1000 non-null   object\n"," 4    Informativeness     1000 non-null   object\n","dtypes: int64(1), object(4)\n","memory usage: 39.2+ KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"f8294885"},"source":["**Reasoning**:\n","The next step is to apply the `clean_tweet_text` function, which was previously defined, to the ' Tweet Text' column of the newly loaded DataFrame. This will create the 'cleaned_text' column, which is essential for feature extraction. Displaying the head of the DataFrame with both original and cleaned text will verify the successful application of the function.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a2383e0","executionInfo":{"status":"ok","timestamp":1766855435102,"user_tz":-480,"elapsed":48,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"a9432464-9473-4da0-b4ac-afd385e12abd"},"source":["import re\n","import string\n","\n","def clean_tweet_text(text):\n","    # 1a. Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    # 1b. Remove user mentions\n","    text = re.sub(r'@\\w+', '', text)\n","    # 1c. Remove hashtags\n","    text = re.sub(r'#\\w+', '', text)\n","    # 1d. Remove punctuation using string.punctuation and re.escape\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n","    # 1e. Convert the text to lowercase\n","    text = text.lower()\n","    # 1f. Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Apply the cleaning function to the ' Tweet Text' column\n","df['cleaned_text'] = df[' Tweet Text'].apply(clean_tweet_text)\n","\n","# Display the first few rows of the DataFrame with original and cleaned text\n","print(\"First 5 rows with original and cleaned tweet text from new dataset:\")\n","print(df[[' Tweet Text', 'cleaned_text']].head())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows with original and cleaned tweet text from new dataset:\n","                                          Tweet Text  \\\n","0  Philippine floods worsen as death toll hits 14...   \n","1  RT @gmanews: Philippine floods worsen as death...   \n","2  Philippine flood fatalities hit 23 http://t.co...   \n","3  4 Luzon dams release water; flood warning up: ...   \n","4  RT @ANCALERTS: PAGASA 8:50pm advisory: Yellow ...   \n","\n","                                        cleaned_text  \n","0  philippine floods worsen as death toll hits 14...  \n","1  rt philippine floods worsen as death toll hits...  \n","2                 philippine flood fatalities hit 23  \n","3  4 luzon dams release water flood warning up ma...  \n","4  rt pagasa 850pm advisory yellow warning for me...  \n"]}]},{"cell_type":"markdown","metadata":{"id":"26521688"},"source":["**Reasoning**:\n","To proceed with the task, I will define the mapping for the new classification categories, apply this mapping to the 'Informativeness' column to create 'new_labels', and then numerically encode these new labels into `y_new` using a `LabelEncoder`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bba68535","executionInfo":{"status":"ok","timestamp":1766855455669,"user_tz":-480,"elapsed":1036,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"5ee4e461-32ed-476b-c20e-50a404c59ff2"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# 1. Create a dictionary that maps the original 'Informativeness' labels to the new categories\n","label_mapping = {\n","    'Related and informative': 'Disaster-Related',\n","    'Not related': 'Not Disaster-Related',\n","    'Not applicable': 'Not Disaster-Related',\n","    'Related - but not informative': 'Uncertain / Needs Review'\n","}\n","\n","# 2. Apply this mapping to the ' Informativeness' column to create a new column named `new_labels`\n","df['new_labels'] = df[' Informativeness'].map(label_mapping)\n","\n","# 3. Initialize a new LabelEncoder\n","new_label_encoder = LabelEncoder()\n","\n","# 4. Fit and transform the `new_labels` column to convert string labels into numerical values\n","y_new = new_label_encoder.fit_transform(df['new_labels'])\n","\n","# 5. Print the value counts of the `new_labels` column\n","print(\"\\nValue counts for the new_labels column:\")\n","print(df['new_labels'].value_counts())\n","\n","# 6. Print the first few entries of `y_new` and the mapping of the new numerical labels\n","print(\"\\nFirst 5 encoded new target labels (y_new):\")\n","print(y_new[:5])\n","print(\"\\nOriginal new labels and their encoded values:\")\n","for i, label in enumerate(new_label_encoder.classes_):\n","    print(f\"{label}: {i}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Value counts for the new_labels column:\n","new_labels\n","Disaster-Related            761\n","Uncertain / Needs Review    145\n","Not Disaster-Related         94\n","Name: count, dtype: int64\n","\n","First 5 encoded new target labels (y_new):\n","[0 0 0 0 0]\n","\n","Original new labels and their encoded values:\n","Disaster-Related: 0\n","Not Disaster-Related: 1\n","Uncertain / Needs Review: 2\n"]}]},{"cell_type":"markdown","metadata":{"id":"c1cb14db"},"source":["**Reasoning**:\n","To prepare the text features for model training with the new dataset, I will initialize a new `TfidfVectorizer`, fit it to the `cleaned_text` column of the updated DataFrame, and then transform the text data into numerical features, storing them in `X`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84c3dc58","executionInfo":{"status":"ok","timestamp":1766855477651,"user_tz":-480,"elapsed":118,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"2667832e-ab60-4610-ce96-44c5f3729e12"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Initialize TfidfVectorizer with the same parameters as before\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer on the 'cleaned_text' from the *new* dataset\n","tfidf_vectorizer.fit(df['cleaned_text'])\n","\n","# Transform the 'cleaned_text' column into TF-IDF features\n","X = tfidf_vectorizer.transform(df['cleaned_text'])\n","\n","# Print the shape of the newly generated X\n","print(\"Shape of TF-IDF features (X):\")\n","print(X.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of TF-IDF features (X):\n","(1000, 2688)\n"]}]},{"cell_type":"markdown","metadata":{"id":"f4c785e5"},"source":["**Reasoning**:\n","Now that the TF-IDF features (X) and the new target labels (y_new) are ready, I will split them into training and testing sets and then initialize and train a Logistic Regression model on these sets.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4ee9abe","executionInfo":{"status":"ok","timestamp":1766855526333,"user_tz":-480,"elapsed":520,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"4f144ced-b400-4a78-9f47-031374d46996"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","# Split the dataset into training and testing sets using X and y_new\n","X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=0.2, random_state=42)\n","\n","# Initialize and train a Logistic Regression model\n","# Using 'liblinear' solver for better convergence with small datasets and sparse data\n","new_model = LogisticRegression(solver='liblinear', random_state=42)\n","new_model.fit(X_train, y_train)\n","\n","print(\"Shapes of the split data for new labels:\")\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shapes of the split data for new labels:\n","X_train shape: (800, 2688)\n","X_test shape: (200, 2688)\n","y_train shape: (800,)\n","y_test shape: (200,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"5f54b292"},"source":["**Reasoning**:\n","To evaluate the performance of the newly trained model with the updated dataset, I will make predictions on the test set, calculate its accuracy, and generate a detailed classification report, handling zero division in the report.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9811a4bd","executionInfo":{"status":"ok","timestamp":1766855560288,"user_tz":-480,"elapsed":36,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"202eb960-5849-4e29-b380-c80399a6d165"},"source":["from sklearn.metrics import accuracy_score, classification_report\n","\n","# Make predictions on the test set using the new_model\n","y_pred_new = new_model.predict(X_test)\n","\n","# Calculate and print accuracy for the new model\n","accuracy_new = accuracy_score(y_test, y_pred_new)\n","print(f\"New Model Accuracy: {accuracy_new:.4f}\")\n","\n","# Generate and print a detailed classification report for the new model\n","print(\"\\nNew Classification Report:\")\n","print(classification_report(y_test, y_pred_new, target_names=new_label_encoder.classes_, zero_division=0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["New Model Accuracy: 0.7600\n","\n","New Classification Report:\n","                          precision    recall  f1-score   support\n","\n","        Disaster-Related       0.76      0.99      0.86       144\n","    Not Disaster-Related       1.00      0.09      0.17        22\n","Uncertain / Needs Review       0.64      0.21      0.31        34\n","\n","                accuracy                           0.76       200\n","               macro avg       0.80      0.43      0.45       200\n","            weighted avg       0.77      0.76      0.69       200\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"fa65a749"},"source":["**Reasoning**:\n","To ensure the updated model and vectorizer can be deployed, I will save the re-fitted `tfidf_vectorizer`, the newly trained `new_model`, and the re-fitted `new_label_encoder` to pickle files, overwriting the previous versions.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3f1d00c","executionInfo":{"status":"ok","timestamp":1766855585136,"user_tz":-480,"elapsed":60,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"0331a277-583c-4b4f-fe8c-17b958479cd0"},"source":["import pickle\n","from sklearn.feature_extraction.text import TfidfVectorizer # Ensure TfidfVectorizer is available\n","\n","# Re-initialize and re-fit TfidfVectorizer before saving to ensure it's in a fitted state.\n","# This explicitly addresses the 'NotFittedError' by guaranteeing the vectorizer is fitted\n","# at the point of serialization, regardless of previous cell execution order or state.\n","# Using the same parameters (max_features) as in previous fitting steps.\n","tfidf_vectorizer_to_save = TfidfVectorizer(max_features=5000)\n","tfidf_vectorizer_to_save.fit(df['cleaned_text'])\n","\n","\n","# 1. Save the tfidf_vectorizer object (using the newly fitted one)\n","with open('tfidf_vectorizer_new.pkl', 'wb') as f:\n","    pickle.dump(tfidf_vectorizer_to_save, f)\n","print(\"Saved tfidf_vectorizer_new.pkl\")\n","\n","# 2. Save the newly trained new_model object\n","with open('new_model.pkl', 'wb') as f:\n","    pickle.dump(new_model, f)\n","print(\"Saved new_model.pkl\")\n","\n","# 3. Save the new_label_encoder object\n","with open('new_label_encoder.pkl', 'wb') as f:\n","    pickle.dump(new_label_encoder, f)\n","print(\"Saved new_label_encoder.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved tfidf_vectorizer_new.pkl\n","Saved new_model.pkl\n","Saved new_label_encoder.pkl\n"]}]},{"cell_type":"markdown","metadata":{"id":"7a511047"},"source":["## Final Task\n","\n","### Subtask:\n","Summarize the process of replacing the dataset, retraining the model, and confirm the updated performance and readiness of the Streamlit application with the new model components.\n"]},{"cell_type":"markdown","metadata":{"id":"767f8c14"},"source":["## Summary:\n","\n","### Q&A\n","*   **Updated performance and readiness of the Streamlit application with the new model components:** The new model achieved an accuracy of 0.7600. While the model shows good performance for the majority 'Disaster-Related' class, the performance for 'Not Disaster-Related' and 'Uncertain / Needs Review' classes is significantly lower, particularly in recall. All necessary model components (TF-IDF vectorizer, trained model, and label encoder) have been successfully saved to pickle files, making them ready for integration into a Streamlit application. However, the application's readiness for reliable classification across all categories is impacted by the model's imbalanced performance.\n","\n","### Data Analysis Key Findings\n","*   The new dataset, containing 1000 tweets from the 2012 Philippines floods, was successfully loaded and processed.\n","*   Tweet texts were cleaned by removing URLs, mentions, hashtags, punctuation, and converting to lowercase, resulting in a `cleaned_text` column.\n","*   The original 'Informativeness' labels were re-categorized into 'Disaster-Related' (761 instances), 'Uncertain / Needs Review' (145 instances), and 'Not Disaster-Related' (94 instances), and then numerically encoded.\n","*   A `TfidfVectorizer` was re-fitted on the new cleaned text, generating 2688 TF-IDF features for each tweet.\n","*   A Logistic Regression model was trained on the new dataset and achieved an accuracy of 0.7600 on the test set.\n","*   The model demonstrated strong performance for the 'Disaster-Related' class (0.76 precision, 0.99 recall), but considerably weaker performance for 'Not Disaster-Related' (1.00 precision, 0.09 recall) and 'Uncertain / Needs Review' (0.64 precision, 0.21 recall), indicating difficulty in identifying minority classes.\n","*   The re-fitted TF-IDF vectorizer, the newly trained Logistic Regression model, and the new label encoder were successfully serialized and saved as pickle files, 'tfidf\\_vectorizer\\_new.pkl', 'new\\_model.pkl', and 'new\\_label\\_encoder.pkl' respectively.\n","\n","### Insights or Next Steps\n","*   The current model exhibits a significant class imbalance issue, leading to poor recall for minority classes ('Not Disaster-Related' and 'Uncertain / Needs Review'). Future work should focus on addressing this imbalance through techniques like oversampling (SMOTE), undersampling, or using weighted loss functions during model training to improve performance on these critical categories.\n","*   To confirm full readiness for a production Streamlit application, it would be beneficial to perform end-to-end testing with the saved components, including loading the artifacts, processing new unseen text data, and making predictions to ensure seamless integration and functionality.\n"]},{"cell_type":"markdown","metadata":{"id":"9c5d975c"},"source":["# Task\n","Load both the '/content/2013_Typhoon_Yolanda-tweets_labeled.csv' and '/content/2012_Philipinnes_floods-tweets_labeled.csv' datasets into separate pandas DataFrames."]},{"cell_type":"markdown","metadata":{"id":"6849fb19"},"source":["## Load Both Datasets\n","\n","### Subtask:\n","Load the '/content/2013_Typhoon_Yolanda-tweets_labeled.csv' and '/content/2012_Philipinnes_floods-tweets_labeled.csv' datasets into separate pandas DataFrames.\n"]},{"cell_type":"markdown","metadata":{"id":"2a7e0139"},"source":["**Reasoning**:\n","Load the two specified CSV files into separate pandas DataFrames, then display their first 5 rows and print their information to understand their structures.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdb311c6","executionInfo":{"status":"ok","timestamp":1766855868414,"user_tz":-480,"elapsed":49,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"f2cbf4c1-d55a-473f-9171-070481305ef8"},"source":["import pandas as pd\n","\n","# Load the '/content/2013_Typhoon_Yolanda-tweets_labeled.csv' file into a DataFrame named df_yolanda\n","df_yolanda = pd.read_csv('/content/2013_Typhoon_Yolanda-tweets_labeled.csv')\n","\n","# Load the '/content/2012_Philipinnes_floods-tweets_labeled.csv' file into a DataFrame named df_floods\n","df_floods = pd.read_csv('/content/2012_Philipinnes_floods-tweets_labeled.csv')\n","\n","# Display the first 5 rows of df_yolanda\n","print(\"First 5 rows of df_yolanda:\")\n","print(df_yolanda.head())\n","\n","# Display the first 5 rows of df_floods\n","print(\"\\nFirst 5 rows of df_floods:\")\n","print(df_floods.head())\n","\n","# Print the DataFrame's information for df_yolanda\n","print(\"\\nDataFrame Information for df_yolanda:\")\n","df_yolanda.info()\n","\n","# Print the DataFrame's information for df_floods\n","print(\"\\nDataFrame Information for df_floods:\")\n","df_floods.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows of df_yolanda:\n","             Tweet ID                                         Tweet Text  \\\n","0  397748184581025793  May super typhoon pala this weekend, Paano na ...   \n","1  397781298577559552  RT @ChikkanessAve: Ariella Arida on her swimwe...   \n","2  397915554066857985  Tropical storm Haiyan intensifies into typhoon...   \n","3  397978405720686592  Super Typhoon Yolanda be good sa mga taga Boho...   \n","4  398015038759247872  RT @MsCandice_UC: Advisory- As of 2am Nov 6 #Y...   \n","\n","   Information Source      Information Type                Informativeness  \n","0           Outsiders  Sympathy and support  Related - but not informative  \n","1         Not labeled           Not labeled                    Not related  \n","2               Media    Caution and advice        Related and informative  \n","3           Outsiders  Sympathy and support  Related - but not informative  \n","4           Outsiders    Caution and advice        Related and informative  \n","\n","First 5 rows of df_floods:\n","             Tweet ID                                         Tweet Text  \\\n","0  230634706436227073  Philippine floods worsen as death toll hits 14...   \n","1  230692092911964161  RT @gmanews: Philippine floods worsen as death...   \n","2  230882841498628096  Philippine flood fatalities hit 23 http://t.co...   \n","3  232034974851805187  4 Luzon dams release water; flood warning up: ...   \n","4  232098107490439169  RT @ANCALERTS: PAGASA 8:50pm advisory: Yellow ...   \n","\n","   Information Source          Information Type          Informativeness  \n","0               Media      Affected individuals  Related and informative  \n","1               Media      Affected individuals  Related and informative  \n","2               Media  Other Useful Information  Related and informative  \n","3               Media        Caution and advice  Related and informative  \n","4               Media        Caution and advice  Related and informative  \n","\n","DataFrame Information for df_yolanda:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1048 entries, 0 to 1047\n","Data columns (total 5 columns):\n"," #   Column               Non-Null Count  Dtype \n","---  ------               --------------  ----- \n"," 0   Tweet ID             1048 non-null   int64 \n"," 1    Tweet Text          1048 non-null   object\n"," 2    Information Source  1048 non-null   object\n"," 3    Information Type    1048 non-null   object\n"," 4    Informativeness     1048 non-null   object\n","dtypes: int64(1), object(4)\n","memory usage: 41.1+ KB\n","\n","DataFrame Information for df_floods:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 5 columns):\n"," #   Column               Non-Null Count  Dtype \n","---  ------               --------------  ----- \n"," 0   Tweet ID             1000 non-null   int64 \n"," 1    Tweet Text          1000 non-null   object\n"," 2    Information Source  1000 non-null   object\n"," 3    Information Type    1000 non-null   object\n"," 4    Informativeness     1000 non-null   object\n","dtypes: int64(1), object(4)\n","memory usage: 39.2+ KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"c07e8992"},"source":["## Combine Datasets\n","\n","### Subtask:\n","Concatenate both loaded DataFrames into a single DataFrame. Ensure column consistency before combining.\n"]},{"cell_type":"markdown","metadata":{"id":"6caf2c31"},"source":["**Reasoning**:\n","To combine the two loaded DataFrames, I will use the `pd.concat` function. Then, I will reset the index of the resulting combined DataFrame and display its head and information to confirm the merge and structure.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ae29587f","executionInfo":{"status":"ok","timestamp":1766855900380,"user_tz":-480,"elapsed":69,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"2aeb8f98-3a19-453e-d568-7e84e24230e0"},"source":["import pandas as pd\n","\n","# 1. Concatenate df_yolanda and df_floods into a new DataFrame\n","df_combined = pd.concat([df_yolanda, df_floods], ignore_index=True)\n","\n","# 2. Reset the index of the df_combined DataFrame, dropping the old index\n","df_combined = df_combined.reset_index(drop=True)\n","\n","# 3. Display the first 5 rows of df_combined\n","print(\"First 5 rows of df_combined:\")\n","print(df_combined.head())\n","\n","# 4. Print the DataFrame's information for df_combined\n","print(\"\\nDataFrame Information for df_combined:\")\n","df_combined.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows of df_combined:\n","             Tweet ID                                         Tweet Text  \\\n","0  397748184581025793  May super typhoon pala this weekend, Paano na ...   \n","1  397781298577559552  RT @ChikkanessAve: Ariella Arida on her swimwe...   \n","2  397915554066857985  Tropical storm Haiyan intensifies into typhoon...   \n","3  397978405720686592  Super Typhoon Yolanda be good sa mga taga Boho...   \n","4  398015038759247872  RT @MsCandice_UC: Advisory- As of 2am Nov 6 #Y...   \n","\n","   Information Source      Information Type                Informativeness  \n","0           Outsiders  Sympathy and support  Related - but not informative  \n","1         Not labeled           Not labeled                    Not related  \n","2               Media    Caution and advice        Related and informative  \n","3           Outsiders  Sympathy and support  Related - but not informative  \n","4           Outsiders    Caution and advice        Related and informative  \n","\n","DataFrame Information for df_combined:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 2048 entries, 0 to 2047\n","Data columns (total 5 columns):\n"," #   Column               Non-Null Count  Dtype \n","---  ------               --------------  ----- \n"," 0   Tweet ID             2048 non-null   int64 \n"," 1    Tweet Text          2048 non-null   object\n"," 2    Information Source  2048 non-null   object\n"," 3    Information Type    2048 non-null   object\n"," 4    Informativeness     2048 non-null   object\n","dtypes: int64(1), object(4)\n","memory usage: 80.1+ KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"a2f475a4"},"source":["## Preprocess Combined Tweet Text\n","\n","### Subtask:\n","Apply the `clean_tweet_text` function to the ' Tweet Text' column of the combined DataFrame to create a unified 'cleaned_text' column. This involves removing URLs, user mentions, hashtags, punctuation, and converting text to lowercase.\n"]},{"cell_type":"markdown","metadata":{"id":"1149d9dd"},"source":["**Reasoning**:\n","To preprocess the combined tweet text, I will re-define the `clean_tweet_text` function to ensure it's available, then apply it to the ' Tweet Text' column of `df_combined` to create the 'cleaned_text' column, and finally display the head of the DataFrame to verify the cleaning.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b22dddcb","executionInfo":{"status":"ok","timestamp":1766855930762,"user_tz":-480,"elapsed":81,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"c5789a78-e691-4450-9ed9-3b4169cd0fef"},"source":["import re\n","import string\n","\n","def clean_tweet_text(text):\n","    # 1a. Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    # 1b. Remove user mentions\n","    text = re.sub(r'@\\w+', '', text)\n","    # 1c. Remove hashtags\n","    text = re.sub(r'#\\w+', '', text)\n","    # 1d. Remove punctuation using string.punctuation and re.escape\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n","    # 1e. Convert the text to lowercase\n","    text = text.lower()\n","    # 1f. Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Apply the cleaning function to the ' Tweet Text' column of df_combined\n","df_combined['cleaned_text'] = df_combined[' Tweet Text'].apply(clean_tweet_text)\n","\n","# Display the first few rows of the DataFrame with original and cleaned text\n","print(\"First 5 rows of df_combined with original and cleaned tweet text:\")\n","print(df_combined[[' Tweet Text', 'cleaned_text']].head())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows of df_combined with original and cleaned tweet text:\n","                                          Tweet Text  \\\n","0  May super typhoon pala this weekend, Paano na ...   \n","1  RT @ChikkanessAve: Ariella Arida on her swimwe...   \n","2  Tropical storm Haiyan intensifies into typhoon...   \n","3  Super Typhoon Yolanda be good sa mga taga Boho...   \n","4  RT @MsCandice_UC: Advisory- As of 2am Nov 6 #Y...   \n","\n","                                        cleaned_text  \n","0  may super typhoon pala this weekend paano na k...  \n","1  rt ariella arida on her swimwear during the mi...  \n","2  tropical storm haiyan intensifies into typhoon...  \n","3  super typhoon yolanda be good sa mga taga boho...  \n","4  rt advisory as of 2am nov 6 hasnt entered phil...  \n"]}]},{"cell_type":"markdown","metadata":{"id":"c656c908"},"source":["## Map and Encode Labels from Combined Data\n","\n","### Subtask:\n","Apply the predefined label mapping to the 'Informativeness' column of the combined DataFrame to create 'new_labels', and then numerically encode these labels into `y_new` using a `LabelEncoder`.\n"]},{"cell_type":"markdown","metadata":{"id":"2578ee6b"},"source":["**Reasoning**:\n","To apply the new classification mapping and encode the labels numerically, I will define the mapping dictionary, create the 'new_labels' column in the combined DataFrame, then initialize and use a LabelEncoder to transform these into numerical values, and finally display the value counts and mapping.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bd6343ee","executionInfo":{"status":"ok","timestamp":1766855970357,"user_tz":-480,"elapsed":26,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"2e964cfa-b218-4189-a265-905b2d9b8777"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# 1. Create a dictionary that maps the original 'Informativeness' labels to the new categories\n","label_mapping = {\n","    'Related and informative': 'Disaster-Related',\n","    'Not related': 'Not Disaster-Related',\n","    'Not applicable': 'Not Disaster-Related',\n","    'Related - but not informative': 'Uncertain / Needs Review'\n","}\n","\n","# 2. Apply this mapping to the ' Informativeness' column of the df_combined DataFrame to create a new column named `new_labels`\n","df_combined['new_labels'] = df_combined[' Informativeness'].map(label_mapping)\n","\n","# 3. Initialize a new LabelEncoder\n","new_label_encoder = LabelEncoder()\n","\n","# 4. Fit and transform the `new_labels` column to convert string labels into numerical values\n","y_new = new_label_encoder.fit_transform(df_combined['new_labels'])\n","\n","# 5. Print the value counts of the `new_labels` column\n","print(\"\\nValue counts for the new_labels column in df_combined:\")\n","print(df_combined['new_labels'].value_counts())\n","\n","# 6. Print the first few entries of `y_new` and the mapping of the new numerical labels\n","print(\"\\nFirst 5 encoded new target labels (y_new):\")\n","print(y_new[:5])\n","print(\"\\nOriginal new labels and their encoded values:\")\n","for i, label in enumerate(new_label_encoder.classes_):\n","    print(f\"{label}: {i}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Value counts for the new_labels column in df_combined:\n","new_labels\n","Disaster-Related            1526\n","Uncertain / Needs Review     320\n","Not Disaster-Related         202\n","Name: count, dtype: int64\n","\n","First 5 encoded new target labels (y_new):\n","[2 1 0 2 0]\n","\n","Original new labels and their encoded values:\n","Disaster-Related: 0\n","Not Disaster-Related: 1\n","Uncertain / Needs Review: 2\n"]}]},{"cell_type":"markdown","metadata":{"id":"047654fe"},"source":["## Prepare TF-IDF Features from Combined Data\n","\n","### Subtask:\n","Initialize a new `TfidfVectorizer` and fit it on the `cleaned_text` column from the *combined* dataset. Then, transform the `cleaned_text` into TF-IDF features, storing the result in `X`.\n"]},{"cell_type":"markdown","metadata":{"id":"4d281626"},"source":["**Reasoning**:\n","To prepare the text features for model training, I will initialize a new `TfidfVectorizer`, fit it to the `cleaned_text` column of the combined DataFrame, and then transform the text data into numerical features, storing them in `X`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7f53929","executionInfo":{"status":"ok","timestamp":1766856015061,"user_tz":-480,"elapsed":48,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"aaa100ae-1b7d-4a99-cb5a-c25dc5500558"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Initialize TfidfVectorizer with max_features=5000\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","\n","# Fit the vectorizer on the 'cleaned_text' from the combined dataset\n","tfidf_vectorizer.fit(df_combined['cleaned_text'])\n","\n","# Transform the 'cleaned_text' column into TF-IDF features\n","X = tfidf_vectorizer.transform(df_combined['cleaned_text'])\n","\n","# Print the shape of the newly generated X\n","print(\"Shape of TF-IDF features (X):\")\n","print(X.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of TF-IDF features (X):\n","(2048, 4841)\n"]}]},{"cell_type":"markdown","metadata":{"id":"d6ba942f"},"source":["## Train New Classification Model with Combined Data\n","\n","### Subtask:\n","Split the X (TF-IDF features) and y_new (new target labels) from the combined dataset into training and testing sets. Initialize and train a Logistic Regression model using this combined training data.\n"]},{"cell_type":"markdown","metadata":{"id":"e4d71bc2"},"source":["**Reasoning**:\n","To split the combined TF-IDF features and new target labels into training and testing sets, and then train a Logistic Regression model, I will use `train_test_split` and `LogisticRegression` from scikit-learn, and print the shapes of the resulting datasets.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11cbbfae","executionInfo":{"status":"ok","timestamp":1766856044391,"user_tz":-480,"elapsed":19,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"aa13b8d6-ed52-4b26-e960-8c953ba2e9a4"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","# Split the dataset into training and testing sets using X and y_new from the combined dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=0.2, random_state=42)\n","\n","# Initialize and train a Logistic Regression model\n","# Using 'liblinear' solver for better convergence with small datasets and sparse data\n","new_combined_model = LogisticRegression(solver='liblinear', random_state=42)\n","new_combined_model.fit(X_train, y_train)\n","\n","print(\"Shapes of the split data for new labels from combined dataset:\")\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shapes of the split data for new labels from combined dataset:\n","X_train shape: (1638, 4841)\n","X_test shape: (410, 4841)\n","y_train shape: (1638,)\n","y_test shape: (410,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"f278fdbf"},"source":["## Evaluate New Model Performance with Combined Data\n","\n","### Subtask:\n","Make predictions on the test set using the model trained with combined data. Evaluate its performance using accuracy and a detailed classification report, handling 'zero_division' appropriately.\n"]},{"cell_type":"markdown","metadata":{"id":"f78484da"},"source":["**Reasoning**:\n","To evaluate the performance of the newly trained model with combined data, I will make predictions on the test set, calculate its accuracy, and generate a detailed classification report, handling zero division in the report.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d15bcf35","executionInfo":{"status":"ok","timestamp":1766856094004,"user_tz":-480,"elapsed":40,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"38641825-9d47-4215-ca8a-16174ddfdf66"},"source":["from sklearn.metrics import accuracy_score, classification_report\n","\n","# Make predictions on the test set using the new_combined_model\n","y_pred_new = new_combined_model.predict(X_test)\n","\n","# Calculate and print accuracy for the new model\n","accuracy_new = accuracy_score(y_test, y_pred_new)\n","print(f\"New Combined Model Accuracy: {accuracy_new:.4f}\")\n","\n","# Generate and print a detailed classification report for the new model\n","print(\"\\nNew Combined Model Classification Report:\")\n","print(classification_report(y_test, y_pred_new, target_names=new_label_encoder.classes_, zero_division=0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["New Combined Model Accuracy: 0.7780\n","\n","New Combined Model Classification Report:\n","                          precision    recall  f1-score   support\n","\n","        Disaster-Related       0.79      0.98      0.88       310\n","    Not Disaster-Related       0.44      0.10      0.16        40\n","Uncertain / Needs Review       0.67      0.17      0.27        60\n","\n","                accuracy                           0.78       410\n","               macro avg       0.63      0.42      0.44       410\n","            weighted avg       0.74      0.78      0.72       410\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"c8ceac63"},"source":["## Save Updated Model Components\n","\n","### Subtask:\n","Save the re-fitted `tfidf_vectorizer`, the newly trained `new_model`, and the re-fitted `new_label_encoder` to pickle files ('tfidf_vectorizer_new.pkl', 'new_model.pkl', 'new_label_encoder.pkl').\n"]},{"cell_type":"markdown","metadata":{"id":"10929603"},"source":["**Reasoning**:\n","To save the necessary model components for deployment, I will use the `pickle` library to serialize and save the `tfidf_vectorizer`, the `new_combined_model`, and the `new_label_encoder` to their respective `.pkl` files.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20a5a010","executionInfo":{"status":"ok","timestamp":1766856139665,"user_tz":-480,"elapsed":45,"user":{"displayName":"Ramos, Railey Mar","userId":"16412237081671672773"}},"outputId":"f472c0bb-f426-439d-b6d0-72acb2cff118"},"source":["import pickle\n","\n","# 1. Save the tfidf_vectorizer object\n","with open('tfidf_vectorizer_new.pkl', 'wb') as f:\n","    pickle.dump(tfidf_vectorizer, f)\n","print(\"Saved tfidf_vectorizer_new.pkl\")\n","\n","# 2. Save the newly trained new_combined_model object\n","with open('new_model.pkl', 'wb') as f:\n","    pickle.dump(new_combined_model, f)\n","print(\"Saved new_model.pkl\")\n","\n","# 3. Save the new_label_encoder object\n","with open('new_label_encoder.pkl', 'wb') as f:\n","    pickle.dump(new_label_encoder, f)\n","print(\"Saved new_label_encoder.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved tfidf_vectorizer_new.pkl\n","Saved new_model.pkl\n","Saved new_label_encoder.pkl\n"]}]},{"cell_type":"markdown","metadata":{"id":"8a2466f7"},"source":["## Final Task\n","\n","### Subtask:\n","Summarize the process of replacing the dataset, retraining the model, and confirm the updated performance and readiness of the Streamlit application with the new model components.\n","\n","## Summary:\n","\n","### Q&A\n","*   **Updated performance and readiness of the Streamlit application with the new model components:** The new model achieved an accuracy of 0.7780. While the model shows good performance for the majority 'Disaster-Related' class, the performance for 'Not Disaster-Related' and 'Uncertain / Needs Review' classes is significantly lower, particularly in recall. All necessary model components (TF-IDF vectorizer, trained model, and label encoder) have been successfully saved to pickle files, making them ready for integration into a Streamlit application. The Streamlit app ('app.py') has been updated to load these new components, ensuring that it uses the model trained on the combined dataset for predictions. However, the application's readiness for reliable classification across all categories is still impacted by the model's imbalanced performance on minority classes.\n","\n","### Data Analysis Key Findings\n","*   Both the 2013 Typhoon Yolanda and 2012 Philippines floods datasets were successfully loaded and combined into a single DataFrame of 2048 tweets.\n","*   Tweet texts from the combined dataset were cleaned by removing URLs, mentions, hashtags, punctuation, and converting to lowercase, resulting in a unified `cleaned_text` column.\n","*   The original 'Informativeness' labels from the combined dataset were re-categorized into 'Disaster-Related' (1526 instances), 'Uncertain / Needs Review' (320 instances), and 'Not Disaster-Related' (202 instances), and then numerically encoded.\n","*   A `TfidfVectorizer` was re-initialized and re-fitted on the `cleaned_text` from the *combined* dataset, generating 4841 TF-IDF features for each tweet.\n","*   A Logistic Regression model was trained on the combined dataset and achieved an accuracy of 0.7780 on the test set.\n","*   The model demonstrated strong performance for the 'Disaster-Related' class (0.79 precision, 0.98 recall), but considerably weaker performance for 'Not Disaster-Related' (0.44 precision, 0.10 recall) and 'Uncertain / Needs Review' (0.67 precision, 0.17 recall), indicating persistent difficulty in identifying minority classes, despite the larger dataset.\n","*   The re-fitted TF-IDF vectorizer, the newly trained Logistic Regression model, and the new label encoder were successfully serialized and saved as pickle files, 'tfidf_vectorizer_new.pkl', 'new_model.pkl', and 'new_label_encoder.pkl' respectively.\n","\n","### Insights or Next Steps\n","*   The current model still exhibits a significant class imbalance issue, leading to poor recall for minority classes ('Not Disaster-Related' and 'Uncertain / Needs Review'). Future work should implement more robust techniques to address this imbalance, such as advanced sampling methods (e.g., SMOTE-NC for categorical features), ensemble methods, or exploration of more complex models like deep learning that can better handle sparse and imbalanced data.\n","*   Further analysis of misclassified samples for minority classes could reveal patterns or specific features that are not being adequately captured by the current TF-IDF approach, suggesting a need for different feature engineering strategies or external knowledge integration.\n","*   Conduct thorough end-to-end testing of the Streamlit application with the updated model components to ensure stability, responsiveness, and correct functionality in a deployed environment."]},{"cell_type":"markdown","metadata":{"id":"4e2f963f"},"source":["## Final Task\n","\n","### Subtask:\n","Summarize the process of replacing the dataset, retraining the model, and confirm the updated performance and readiness of the Streamlit application with the new model components.\n","\n","## Summary:\n","\n","### Q&A\n","*   **Updated performance and readiness of the Streamlit application with the new model components:** The new model achieved an accuracy of 0.7780. While the model shows good performance for the majority 'Disaster-Related' class, the performance for 'Not Disaster-Related' and 'Uncertain / Needs Review' classes is significantly lower, particularly in recall. All necessary model components (TF-IDF vectorizer, trained model, and label encoder) have been successfully saved to pickle files, making them ready for integration into a Streamlit application. The Streamlit app ('app.py') has been updated to load these new components, ensuring that it uses the model trained on the combined dataset for predictions. However, the application's readiness for reliable classification across all categories is still impacted by the model's imbalanced performance on minority classes.\n","\n","### Data Analysis Key Findings\n","*   Both the 2013 Typhoon Yolanda and 2012 Philippines floods datasets were successfully loaded and combined into a single DataFrame of 2048 tweets.\n","*   Tweet texts from the combined dataset were cleaned by removing URLs, mentions, hashtags, punctuation, and converting to lowercase, resulting in a unified `cleaned_text` column.\n","*   The original 'Informativeness' labels from the combined dataset were re-categorized into 'Disaster-Related' (1526 instances), 'Uncertain / Needs Review' (320 instances), and 'Not Disaster-Related' (202 instances), and then numerically encoded.\n","*   A `TfidfVectorizer` was re-initialized and re-fitted on the `cleaned_text` from the *combined* dataset, generating 4841 TF-IDF features for each tweet.\n","*   A Logistic Regression model was trained on the combined dataset and achieved an accuracy of 0.7780 on the test set.\n","*   The model demonstrated strong performance for the 'Disaster-Related' class (0.79 precision, 0.98 recall), but considerably weaker performance for 'Not Disaster-Related' (0.44 precision, 0.10 recall) and 'Uncertain / Needs Review' (0.67 precision, 0.17 recall), indicating persistent difficulty in identifying minority classes, despite the larger dataset.\n","*   The re-fitted TF-IDF vectorizer, the newly trained Logistic Regression model, and the new label encoder were successfully serialized and saved as pickle files, 'tfidf_vectorizer_new.pkl', 'new_model.pkl', and 'new_label_encoder.pkl' respectively.\n","\n","### Insights or Next Steps\n","*   The current model still exhibits a significant class imbalance issue, leading to poor recall for minority classes ('Not Disaster-Related' and 'Uncertain / Needs Review'). Future work should implement more robust techniques to address this imbalance, such as advanced sampling methods (e.g., SMOTE-NC for categorical features), ensemble methods, or exploration of more complex models like deep learning that can better handle sparse and imbalanced data.\n","*   Further analysis of misclassified samples for minority classes could reveal patterns or specific features that are not being adequately captured by the current TF-IDF approach, suggesting a need for different feature engineering strategies or external knowledge integration.\n","*   Conduct thorough end-to-end testing of the Streamlit application with the updated model components to ensure stability, responsiveness, and correct functionality in a deployed environment.\n"]},{"cell_type":"markdown","metadata":{"id":"73040022"},"source":["## Final Task\n","\n","### Subtask:\n","Summarize the process of integrating both datasets, retraining the model, and confirming the updated performance and readiness of the Streamlit application with the new model components.\n"]},{"cell_type":"markdown","metadata":{"id":"befad6bd"},"source":["## Summary:\n","\n","### Q&A\n","*   **Updated performance and readiness of the Streamlit application with the new model components:** The new model achieved an accuracy of 0.7780. While the model shows good performance for the majority 'Disaster-Related' class, the performance for 'Not Disaster-Related' and 'Uncertain / Needs Review' classes is significantly lower, particularly in recall. All necessary model components (TF-IDF vectorizer, trained model, and label encoder) have been successfully saved to pickle files, making them ready for integration into a Streamlit application. The Streamlit app (`app.py`) has been updated to load these new components, ensuring that it uses the model trained on the combined dataset for predictions. However, the application's readiness for reliable classification across all categories is still impacted by the model's imbalanced performance on minority classes.\n","\n","### Data Analysis Key Findings\n","*   Both the 2013 Typhoon Yolanda and 2012 Philippines floods datasets were successfully loaded and combined into a single DataFrame of 2048 tweets.\n","*   Tweet texts from the combined dataset were cleaned by removing URLs, mentions, hashtags, punctuation, and converting to lowercase, resulting in a unified `cleaned_text` column.\n","*   The original 'Informativeness' labels from the combined dataset were re-categorized into 'Disaster-Related' (1526 instances), 'Uncertain / Needs Review' (320 instances), and 'Not Disaster-Related' (202 instances), and then numerically encoded.\n","*   A `TfidfVectorizer` was re-initialized and re-fitted on the `cleaned_text` from the *combined* dataset, generating 4841 TF-IDF features for each tweet.\n","*   A Logistic Regression model was trained on the combined dataset and achieved an accuracy of 0.7780 on the test set.\n","*   The model demonstrated strong performance for the 'Disaster-Related' class (0.79 precision, 0.98 recall), but considerably weaker performance for 'Not Disaster-Related' (0.44 precision, 0.10 recall) and 'Uncertain / Needs Review' (0.67 precision, 0.17 recall), indicating persistent difficulty in identifying minority classes, despite the larger dataset.\n","*   The re-fitted TF-IDF vectorizer, the newly trained Logistic Regression model, and the new label encoder were successfully serialized and saved as pickle files, 'tfidf\\_vectorizer\\_new.pkl', 'new\\_model.pkl', and 'new\\_label\\_encoder.pkl' respectively.\n","\n","### Insights or Next Steps\n","*   The current model still exhibits a significant class imbalance issue, leading to poor recall for minority classes ('Not Disaster-Related' and 'Uncertain / Needs Review'). Future work should implement more robust techniques to address this imbalance, such as advanced sampling methods (e.g., SMOTE-NC for categorical features), ensemble methods, or exploration of more complex models like deep learning that can better handle sparse and imbalanced data.\n","*   Further analysis of misclassified samples for minority classes could reveal patterns or specific features that are not being adequately captured by the current TF-IDF approach, suggesting a need for different feature engineering strategies or external knowledge integration.\n"]}]}